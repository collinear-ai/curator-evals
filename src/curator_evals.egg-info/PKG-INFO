Metadata-Version: 2.4
Name: curator-evals
Version: 0.1.0
Summary: A library for evaluating language models on various tasks
Home-page: https://github.com/collinear-ai/curator-evals
Author: Collinear AI
Author-email: info@collinear.ai
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: datasets==3.6.0
Requires-Dist: torch
Requires-Dist: transformers
Requires-Dist: numpy
Requires-Dist: scikit-learn
Requires-Dist: huggingface-hub
Requires-Dist: jinja2
Requires-Dist: bitsandbytes
Requires-Dist: accelerate>=0.26.0
Requires-Dist: peft
Requires-Dist: vllm>=0.3.0
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: requests>=2.31.0
Requires-Dist: ray
Requires-Dist: jupyter
Requires-Dist: matplotlib
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Curator Evals

A library for evaluating language models on various tasks using the Curator Eval Bench dataset.

## Installation

```bash
pip install curator-evals
```

## Usage

The library provides a command-line interface for running evaluations:

```bash
python -m curator_evals.cli --task coherence_evals --model collinear/x --model-type phi
```

### Supported Tasks

- `math_correctness_evals`: Evaluate mathematical reasoning capabilities
- `instruction_following_evals`: Evaluate ability to follow instructions
- `coherence_evals`: Evaluate text coherence
- `instruction_complexity_evals`: Evaluate handling of complex instructions
- `code_correctness_evals`: Evaluate code generation and correctness

### Model Types

- `llm`: Large language models (causal language models)
- `llm_adapter`: Language models with adapters
- `encoder`: Encoder models (reward models, classifiers)
- `ensemble`: Ensemble of multiple models
- `nvidia_complexity`: NVIDIA instruction complexity classifier

### NVIDIA Instruction Complexity Classifier

The library now supports the NVIDIA instruction complexity classifier (`nvidia/prompt-task-and-complexity-classifier`) for detailed instruction analysis. This classifier provides:

- **Complexity Score**: Overall complexity rating (0-1 scale)
- **Task Type Classification**: Primary and secondary task types
- **Component Scores**: 
  - Creativity scope
  - Reasoning requirements
  - Contextual knowledge needed
  - Domain knowledge requirements
  - Constraint complexity
  - Few-shot examples needed

Example usage:

```bash
curator-evals --task instruction_complexity \
--model nvidia/prompt-task-and-complexity-classifier \
--model-type nvidia_complexity \
--debug
```

## Python API

You can also use the library programmatically:

```python
from curator_evals import evaluate

# Run evaluation
metrics = evaluate(
    task="coherence_evals",
    model_info={
        "model_id": "collinear/x",
        "model_type": "phi"
    }
)

# Print results
print(metrics)
```

## Dataset Structure

The evaluation dataset is hosted on HuggingFace Hub at `collinear-ai/curator_evals_bench`. Each task is a subset of the dataset, containing different splits for various dataset sources.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
